{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644f4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from cnn_classifier import logger\n",
    "from cnn_classifier.entity.config_entity import EvaluationConfig\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import json # To save evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a5df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device for model evaluation: {self.device}\")\n",
    "\n",
    "        self.model = self._load_model()\n",
    "        self.test_loader = self._get_test_loader()\n",
    "        self.class_names = self.config.class_names\n",
    "        logger.info(f\"Evaluation component initialized. Model loaded, test loader ready.\")\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the trained model from the specified path.\n",
    "        \"\"\"\n",
    "        # Initialize ResNet18 architecture (without pre-trained weights initially)\n",
    "        model = models.resnet18(weights=None)\n",
    "        # Adjust the final fully connected layer to match your number of classes\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, self.config.num_classes)\n",
    "\n",
    "        # Load the saved state dictionary (trained weights)\n",
    "        model.load_state_dict(torch.load(self.config.model_path, map_location=self.device))\n",
    "        model.to(self.device) # Move model to the appropriate device\n",
    "        model.eval() # Set model to evaluation mode (important for inference)\n",
    "        logger.info(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "        return model\n",
    "\n",
    "    def _get_test_loader(self):\n",
    "        \"\"\"\n",
    "        Prepares the DataLoader for the testing dataset.\n",
    "        This should use the same transformations as the validation/test set during training.\n",
    "        \"\"\"\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(self.config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        cpu_count=os.cpu_count() or 0\n",
    "        test_dataset = datasets.ImageFolder(\n",
    "            root=self.config.testing_data,\n",
    "            transform=test_transform\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False, # No need to shuffle test data\n",
    "            num_workers=cpu_count\n",
    "        )\n",
    "        logger.info(f\"Test DataLoader created with {len(test_loader)} batches from {self.config.testing_data}.\")\n",
    "        return test_loader\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Performs a comprehensive evaluation of the model on the test set\n",
    "        and saves the metrics.\n",
    "        \"\"\"\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "            for inputs, labels in self.test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        report = classification_report(all_labels, all_preds, target_names=self.class_names, output_dict=True)\n",
    "        cm = confusion_matrix(all_labels, all_preds).tolist() # Convert to list for JSON serialization\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm\n",
    "        }\n",
    "\n",
    "        # Create evaluation root directory if it doesn't exist\n",
    "        self.config.root_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # Define path to save metrics\n",
    "        metrics_path = self.config.root_dir / \"evaluation_metrics.json\"\n",
    "\n",
    "        # Save metrics to a JSON file\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "        logger.info(f\"Evaluation metrics saved to: {metrics_path}\")\n",
    "        logger.info(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Classification Report:\\n{json.dumps(report, indent=4)}\")\n",
    "        logger.info(f\"Confusion Matrix:\\n{json.dumps(cm, indent=4)}\")\n",
    "\n",
    "        return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a885eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b2794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19419c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbd678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f4dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd86779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dc8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46682d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_tumour_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
